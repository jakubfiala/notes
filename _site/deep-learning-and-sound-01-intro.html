<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Deep Learning and Sound ~ 01: Intro | fiala ~ notes</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@fiala__" /><meta name="twitter:title" content="Deep Learning and Sound ~ 01: Intro" /><meta name="twitter:description" content="This is the first from a series of posts I’m going to write over the next year or so, detailing my research in audio generation with deep learning techniques.Due to the quick-and-dirty nature of th..."><meta name="description" content="This is the first from a series of posts I’m going to write over the next year or so, detailing my research in audio generation with deep learning techniques..."> <!-- General Meta Tags --><meta charset="UTF-8"> <!-- SEO --> <!-- FB OG--><meta property="og:image" content="/notes/assets/avatar.jpg"><meta property="og:type" content="website"/><meta property="og:title" content="Deep Learning and Sound ~ 01: Intro" /><meta property="og:description" content="This is the first from a series of posts I’m going to write over the next year or so, detailing my research in audio generation with deep learning techniques.Due to the quick-and-dirty nature of th..."/><meta property="og:url" content="http://fiala.uk/notes"> <!-- General --><meta name="title" content="Deep Learning and Sound ~ 01: Intro" /> <!-- Android Web App Meta Tags --><meta name="mobile-web-app-capable" content="yes"><link rel="manifest" href="other/manifest.json"><meta name="theme-color" content=" color"><link rel="icon" href="/notes/assets/favicon.png"><link rel="apple-touch-icon" href="/notes/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/notes/assets/core.css"><link rel="canonical" href="/notes/deep-learning-and-sound-01-intro"><link rel="alternate" type="application/atom+xml" title="fiala ~ notes" href="/notes/feed.xml" /></head><body><aside class="logo"> <a href="/notes/"> <img src="http://www.gravatar.com/avatar/a52dc079f50d7c6c23ea6c20717e58ab.png?s=80" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span><h1 class="big-title">fiala ~ notes</h1></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1>Deep Learning and Sound ~ 01: Intro</h1><time>November 27, 2015</time></div><div class="divider"></div><p>This is the first from a series of posts I’m going to write over the next year or so, detailing my research in audio generation with deep learning techniques.<br /> Due to the quick-and-dirty nature of this blog and my generally inadequate level of knowledge in this area, I’m not going to discuss existing theories and practices<br /> of deep learning, neural networks etc. in much detail – this is rather an attempt to formalize my research and development, and provide readable explanations of the problem solving approaches I’m developing.</p><h4 id="but-i-want-to-find-out-everything-about-deep-learning">“But I want to find out EVERYTHING about deep learning!”</h4><p>If you’d like to learn more about deep learning methods, you should definitely check out Andrej Karpathy’s <a href="https://cs231n.github.io">cs231n</a> course notes, which will gracefully guide you all the way from basic kNN algorithms (ancient stuff), through linear models and multi-layer perceptrons (80s stuff) to convolutional neural networks (21st century stuff). When you feel confident about those, be sure to check out Andrej’s <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">beautiful blog post</a> on Recurrent neural nets, which I’m going to be focusing on <em>a lot.</em> If that doesn’t satiate your appetite for knowledge, the next step is to read some papers – Alex Graves’ handwriting generation<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> is mindblowing, as well as the smoking hot paper which has just been published<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> by Alec Radford et al. There’s a million others, too, and most of them are accessible on <em>arxiv</em> or via Google Scholar. Good luck!</p><hr /><h3 id="back-to-business">… back to business.</h3><p>Having given you enough reading for the next couple of weeks, I can now <del>stop writing this post, order a takeaway pho and play some fallout 4</del> move on to explaining what my research is all about. Here’s the brief:</p><h2 id="what-were-aiming-for">What we’re aiming for</h2><p>In the recent years, deep learning has become extremely hyped and widely used as a method for developing <strong>robust, accurate models of complex data.</strong> In other words, people realised that if your computer is fast enough, you can stack up many, many small mathematical functions (units, neurons, etc.), and using standard, decades-old backpropagation algorithms, tweak these functions so that they end up being capable of <strong>classifying, predicting or transforming</strong> not only simple data vectors such as wind speed records in Aberdeen, but also complex, semantically abstract and high-dimensional data such as images, sound, human-made text, etc.<br /> <br /><br /> <em><strong>Example:</strong> Using deep learning, you can generate pictures of bedrooms that don’t exist:</em><br /><br /> <img src="https://raw.githubusercontent.com/Newmu/dcgan_code/master/images/lsun_bedrooms_five_epoch_samples.png" alt="DCGAN by Alex Radford et al." /><br /> <em>source: DCGAN by Alex Radford et al.</em><br /> <br /><br /> The magic of this technology is that in order to create an AV generation system, one can use example-based training – rather than tweak software parameters that only the computer really understands, one can describe their desired output by exposing the system to a range of things that “sound or look like” the desired output. <strong>The aim of my research, as part of the <a href="http://eavi.goldsmithsdigital.com">EAVI</a> research group at Goldsmiths, is to <em>build tools that harness the power of deep learning to generate musical and non-musical sound.</em></strong> These tools may well prove to be much more intuitive and creative than any existing sound composition tools.</p><h2 id="state-of-the-art">State of the art</h2><p>Now, people have been generating <em>music</em> with neural networks for a while. Notable is the work of Douglas Eck et al.<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>, who managed to train a recurrent neural network (henceforth RNN) on a dataset of blues songs, and generate rather plausible blues compositions. The problem, as usual, lies in the data: rather than feeding their model with recordings of blues songs, they trained it on the <em>structural data</em> of the individual songs – each song was represented as a sequence of note values, and timing was merely implicit, i.e. each step in the sequence is assumed to last the same amount of time.</p><p>As for training on/generating audio data, this has only really been possible with the recent rise of deep models, i.e. models with many layers of units, each performing a small calculation relevant to only a small part of the input data. Methods for doing this are largely inspired by efforts in <strong>speech recognition,</strong> such as Graves et al.<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>, where audio is split into short (typically around 10ms) frames, and for each frame, we calculate feature values, which are then used to train a model. The training is usually performed on one or more high-end GPUs, using frameworks such as Caffe (Lua), Theano (python), or more recently, TensorFlow (python). However, since the final output of speech recognition models is usually a single label, or a set of probabilities for each possible label, it is generally possible to train on dramatically reduced representations of the data, such as MFCC, from which audio signals cannot be reconstructed.</p><p><img src="http://pmtk3.googlecode.com/svn-history/r663/trunk/docs/demos/dataDemos/plotMFCC_02.png" alt="MFCC of 'four'" /><br /> <em>source: Tommi Jaakkola</em></p><p><br /><br /> So the two of the main problems that arise when training models on audio data, the Scylla and Charybdis of neural nets and sound, are:</p><ul><li><strong>high dimensionality</strong> – in one second of CD-quality audio, there are 44100 numbers. However, not much interesting stuff can happen in that time frame. This means that the model needs extremely large chunks of data to obtain a good idea of what’s going on, unlike images from common datasets like CIFAR-10<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>, where only a few pixels may contain valuable semantic information.</li><li><strong>repetition</strong> – even when the model learns from huge frames (2048+ numbers in one example), in natural sounds, each distinctive waveform shape tends to repeat tens, or even hundreds of times, before a significant change occurs. This means that the model needs to be trained with data that barely ever changes, as far as the model’s memory is concerned. This is also why LSTM models outperform standard RNNs in audio-related tasks.<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup> Even when the low-level repetition of waveforms is dealt with, repetition of beats, melodic phrases, and entire sections of songs come into play, particularly when working with recordings of popular music, which is extremely spectrally and semantically rich.</li></ul><p>I’m way too happy about this mythological analogy. Let’s move on.</p><h3 id="fighting-scylla-and-charybdis">Fighting Scylla and Charybdis</h3><p>There have only been a handful of efforts by researchers and coders to tackle the two big problems with audio generation. Perhaps the most notable one that I came across, is a project called <strong>GRUV</strong> by Aran Nayebi and Matt Vitelli<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup>, which is luckily all <a href="https://github.com/MattVitelli/GRUV">on GitHub</a>. In their <a href="https://www.youtube.com/watch?v=0VTI1BBLydE">demo video</a> on YouTube, they demonstrate training a model with a dataset of Madeon songs (which is an abomination), and amazingly, after a good amount of training epochs, one can clearly hear the disgusting compressed EDM beat, traces of discount vocal samples from Beatport, and (thank the heavens!) a considerable amount of random noise.</p><p>After recovering from the musical experience, and looking at the python code, it seems that GRUV represents a good (albeit only partial) solution to the above problems. Its LSTM architecture is capable of recognizing the most important parts of EDM music style, and generating a somewhat meaningful sound pattern. The output does not seem to evolve beyond a steady beat, but in terms of frequency content, it’s absolutely characteristic of the input dataset. More importantly, several smart solutions are introduced in the code, that seem to alleviate the aforementioned problems of audio data.</p><ul><li><strong>Data is preprocessed by subtracting the mean and dividing by variance.</strong> These two are then stored as numpy arrays, and added into the output in the generation script. This means that a lot of the information that is the same in every frame never makes it into the model, but still appears in the output to reconstruct the original sound as closely as possible. The model is exposed to more diverse data, and can learn much easier (the following example is taken directly from the GRUV code).</li></ul><figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Mean across num examples and num timesteps</span>
<span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c"># STD across num examples and num timesteps</span>
<span class="n">std_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x_data</span><span class="o">-</span><span class="n">mean_x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="c"># Clamp variance if too tiny</span>
<span class="n">std_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">1.0e-8</span><span class="p">,</span> <span class="n">std_x</span><span class="p">)</span></code></pre></figure><ul><li><strong>Time-distributed FC layers envelop the LSTM layers in the model.</strong> This means that in both the input and output layer, the same transform is applied to each timestep of the training sequence.</li><li><strong>Data is transformed into the frequency domain via FFT, but the model is trained on both the real and imaginary parts of the transform.</strong> Frequency data, being more representative of what the sound is like to a human, are probably more useful to train on. However, because the model effectively trains on a complex spectrum, it does not lose the phase information, and it’s easier to reconstruct the output in the time domain.</li></ul><p>The last point is somewhat interesting, because the way GRUV implements it, is by concatenating the real vector to the imaginary vector. In the end, we end up with the same dimensionality as if we just fed in the original signal. This, beside the fact that the generated sounds tend to be noisy and repetitive, means that the two main problems – dimensionality and repetition – still need to be solved.</p><hr /><h3 id="conclusion">Conclusion</h3><p>In the next post, I’m going to discuss a few approaches I’m exploring that might help. My aim is to train with simpler audio material, such as drum &amp; mallet samples, although initially I’m going to run tests with artificial waveforms. The short-term goal is to create a system that can generate a <strong>relatively clean</strong>, <strong>smooth</strong>, but <strong>dynamic</strong> audio sequence, which can then be used as an instrument, rather than a finished audio track.</p><div class="footnotes"><ol><li id="fn:1"><p><a href="http://arxiv.org/abs/1308.0850">http://arxiv.org/abs/1308.0850</a> <a href="#fnref:1" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:2"><p><a href="http://arxiv.org/abs/1511.06434">http://arxiv.org/abs/1511.06434</a> <a href="#fnref:2" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:3"><p><a href="ftp://ftp.idsia.ch/pub/juergen/2002_icannMusic.pdf">ftp://ftp.idsia.ch/pub/juergen/2002_icannMusic.pdf</a> <a href="#fnref:3" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:4"><p><a href="http://arxiv.org/pdf/1303.5778">http://arxiv.org/pdf/1303.5778</a> <a href="#fnref:4" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:5"><p><a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a> <a href="#fnref:5" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:6"><p><a href="http://www.cs.toronto.edu/~graves/nn_2005.pdf">http://www.cs.toronto.edu/~graves/nn_2005.pdf</a> <a href="#fnref:6" class="reversefootnote">&#8617;&#xfe0e;</a></p></li><li id="fn:7"><p><a href="https://cs224d.stanford.edu/reports/NayebiAran.pdf">https://cs224d.stanford.edu/reports/NayebiAran.pdf</a> <a href="#fnref:7" class="reversefootnote">&#8617;&#xfe0e;</a></p></li></ol></div></article><div class="back"> <a href="/notes/">Back</a></div></main><aside class="note"></aside><script async src="/notes/assets/footnotes.js"></script></body></html>